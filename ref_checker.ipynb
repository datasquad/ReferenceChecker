{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e53054",
   "metadata": {},
   "source": [
    "# Student submission reference checker\n",
    "\n",
    "Ralf Becker, The University of Manchester\n",
    "\n",
    "This was written with substantial support by ChatGPT (see this [chat](https://chatgpt.com/share/695974b2-c5ec-8010-8a51-bd282f3ba41f)).\n",
    "\n",
    "This code will cycle through all pdf files in a folder (presumably student assignment submissions), identify the references and check whether the references are real or hallucination. This process requires a little confidence in installing softwares and a little familiarity with Terminals.\n",
    "\n",
    "Here is an outline of the process this works through. \n",
    "\n",
    "Download all submissions into a specific folder.\n",
    "\n",
    "The code registers all pdf files in a folder with submissions. For each of the pdf files it does the following:\n",
    "\n",
    "1. Uses the Grobit software to extract a list of references (from a reference list) from the document.\n",
    "2. Attempts to verify each of the extracted references\n",
    "\n",
    "Returns a csv file with one line for each submission - unverified reference combination. This uses the name of the submission files as the index. In Gradescope this will maintain student anonymity of that is switched on in Gradescope.\n",
    "\n",
    "How does it verify a reference?\n",
    "\n",
    "There are basically three ways in which a reference can get verified. \n",
    "\n",
    "a) The reference provided a doi and it is available and has a matching title to the reference provided\n",
    "b) If a) was not successful (or no doi provided) the reference information is matched against two databases ([OpenAlex](https://openalex.org/) and [CrossRef](https://search.crossref.org/)). The reference will basically be verified if the title and the author can be found.\n",
    "c) To be able to verify online articles a reference can be verified using a provided url. If the url provided leads to a page that contains text which contains a heading that matches the title provided in the reference.\n",
    "\n",
    "There are some details that can be tuned but are not described here in detail.\n",
    "\n",
    "**It is important to understand that this algorithm will make both errors. It will identify some references as unverified although they are ok and it will verify some papers as verified despite them being hallucinated.**\n",
    "\n",
    "To run through this process we require some setup.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The software used to extract references is  called [GROBIT](https://grobid.readthedocs.io/en/latest/) which stands for \"GeneRation Of BIbliographic Data\".\n",
    "\n",
    "The GROBIT software is delivered via [Docker](https://www.docker.com/) on your computer. \n",
    "\n",
    "### Installing and Testing Docker\n",
    "\n",
    "Go to [Docker](https://www.docker.com/) to download the Desktop version of Docker that fits your operating system. In my case that was the Windows AMD64 version. Once you downloaded the Docker software to your computer you should go to your Terminal and use the following two commands.\n",
    "\n",
    "``` bash\n",
    "docker --version\n",
    "docker run hello-world\n",
    "```\n",
    "\n",
    "They should give you the version of Docker you have installed and successfully run the short `hello-world` script.\n",
    "\n",
    "### Installing and Testing GROBIT\n",
    "\n",
    "This software uses some pre-trained artificial intelligence engine to extract, from a piece of text, the bibliographic information for the references used.\n",
    "\n",
    "Now you need to install and run the [GROBIT](https://grobid.readthedocs.io/en/latest/) application. The full guidance is available [here](https://grobid.readthedocs.io/en/latest/getting_started/).\n",
    "\n",
    "In your Terminal run\n",
    "\n",
    "``` bash\n",
    "docker pull grobid/grobid:0.8.2.1-crf\n",
    "```\n",
    "\n",
    "This basically loads the software. You can then start it with the following \n",
    "\n",
    "``` bash\n",
    "docker run --rm --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.2.1-crf\n",
    "```\n",
    "\n",
    "If you now open your Docker Desktop you can see the an instance, or a container, with the GROBIT software running.\n",
    "\n",
    "![Docker Desktop](Docker_image1.png)\n",
    "\n",
    "If you click on the port, then a browser will open with the following \n",
    "\n",
    "![Grobit](Grobit_image1.png)\n",
    "\n",
    "Here you can go to the **PDF** tab and upload a PDF document. It highlights the references and the can create hyperlinks to the detected references.\n",
    "\n",
    "You can close that application by clicking on the stop button in the Docker Desktop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e60b5",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "Before you run the code you need to make sure to have the GROBIT container open as described above. `GROBID_URL` refers to that open container. \n",
    "\n",
    "Parameters to set:\n",
    "\n",
    "1. `PDF_FOLDER`, this is the path to the folder in which the pdf submissions are saved.\n",
    "2. `GROBID_URL `, this is the path to the instance in which GROBIT runs. YOu can get it by copying the url you get when you click on the relevant Port in the Docker Desktop (see above)\n",
    "\n",
    "The output will be saved into the source directory of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63623e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from weakref import ref\n",
    "\n",
    "import requests\n",
    "from lxml import etree\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "PDF_FOLDER = \"C:/Temp/Essay\"\n",
    "GROBID_URL = \"http://localhost:8070/api/processReferences\"\n",
    "\n",
    "# Verification APIs\n",
    "CROSSREF_WORKS = \"https://api.crossref.org/works\"\n",
    "OPENALEX_WORKS = \"https://api.openalex.org/works\"\n",
    "\n",
    "# Heuristics / thresholds\n",
    "TITLE_MATCH_THRESHOLD = 85      # fuzzy title match\n",
    "MIN_EVIDENCE_SCORE = 2          # how many checks must pass to call it \"verified\"\n",
    "REQUEST_TIMEOUT = 60\n",
    "SLEEP_BETWEEN_CALLS = 0.2       # be kind to APIs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParsedRef:\n",
    "    title: Optional[str]\n",
    "    year: Optional[int]\n",
    "    authors: List[str]\n",
    "    venue: Optional[str]\n",
    "    doi: Optional[str]\n",
    "    url: Optional[str] = None\n",
    "    raw: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VerificationResult:\n",
    "    verified: bool\n",
    "    score: int\n",
    "    reason: str\n",
    "    matched_source: Optional[str] = None\n",
    "    matched_id: Optional[str] = None\n",
    "    matched_title: Optional[str] = None\n",
    "    matched_doi: Optional[str] = None\n",
    "\n",
    "\n",
    "def grobid_extract_references(pdf_path: str) -> List[ParsedRef]:\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        r = requests.post(\n",
    "            GROBID_URL,\n",
    "            files={\"input\": (os.path.basename(pdf_path), f, \"application/pdf\")},\n",
    "            timeout=REQUEST_TIMEOUT,\n",
    "        )\n",
    "    r.raise_for_status()\n",
    "    tei_xml = r.text.encode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    root = etree.fromstring(tei_xml)\n",
    "    ns = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "    refs = []\n",
    "    for bibl in root.xpath(\".//tei:listBibl/tei:biblStruct\", namespaces=ns):\n",
    "        title = _first_text(bibl.xpath(\".//tei:title[@level='a']/text()\", namespaces=ns)) \\\n",
    "                or _first_text(bibl.xpath(\".//tei:title/text()\", namespaces=ns))\n",
    "\n",
    "        year_txt = _first_text(bibl.xpath(\".//tei:date/@when\", namespaces=ns)) \\\n",
    "                   or _first_text(bibl.xpath(\".//tei:date/text()\", namespaces=ns))\n",
    "        year = _parse_year(year_txt)\n",
    "\n",
    "        doi = _first_text(bibl.xpath(\".//tei:idno[@type='DOI']/text()\", namespaces=ns))\n",
    "        if doi:\n",
    "            doi = doi.strip().lower()\n",
    "            doi = doi.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n",
    "\n",
    "        authors = []\n",
    "        for a in bibl.xpath(\".//tei:author\", namespaces=ns):\n",
    "            surname = _first_text(a.xpath(\".//tei:surname/text()\", namespaces=ns))\n",
    "            forename = _first_text(a.xpath(\".//tei:forename/text()\", namespaces=ns))\n",
    "            if surname and forename:\n",
    "                authors.append(f\"{surname}, {forename}\")\n",
    "            elif surname:\n",
    "                authors.append(surname)\n",
    "\n",
    "        venue = _first_text(bibl.xpath(\".//tei:monogr//tei:title/text()\", namespaces=ns))\n",
    "        \n",
    "        url = (_first_text(bibl.xpath(\".//tei:idno[@type='URI']/text()\", namespaces=ns)) or \\\n",
    "                _first_text(bibl.xpath(\".//tei:idno[@type='url']/text()\", namespaces=ns)) or \\\n",
    "                _first_text(bibl.xpath(\".//tei:ptr/@target\", namespaces=ns)))\n",
    "        if url:\n",
    "            url = url.strip()\n",
    "\n",
    "        # Sometimes Grobid provides an unstructured ref string:\n",
    "        raw = _first_text(bibl.xpath(\".//tei:note[@type='raw_reference']/text()\", namespaces=ns))\n",
    "\n",
    "        refs.append(ParsedRef(title=title, year=year, authors=authors, venue=venue, doi=doi, url=url, raw=raw))\n",
    "\n",
    "    return refs\n",
    "\n",
    "\n",
    "def verify_by_url(ref: ParsedRef) -> Optional[VerificationResult]:\n",
    "    if not ref.url or not ref.title:\n",
    "        return None\n",
    "\n",
    "    url = ref.url.strip()\n",
    "    if not url.lower().startswith((\"http://\", \"https://\")):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Prefer GET (HEAD often blocked or lies)\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            timeout=30,\n",
    "            allow_redirects=True,\n",
    "            headers={\n",
    "                \"User-Agent\": \"ref-checker/0.1 (+https://example.org; contact: you@example.com)\"\n",
    "            },\n",
    "        )\n",
    "        if r.status_code >= 400:\n",
    "            return VerificationResult(False, 0, f\"URL returned HTTP {r.status_code}\", matched_source=\"url\", matched_id=url)\n",
    "\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "\n",
    "        # If the link points to a PDF, you could optionally treat “reachable PDF” as weak evidence,\n",
    "        # or even run Grobid on it later.\n",
    "        if \"application/pdf\" in ctype or url.lower().endswith(\".pdf\"):\n",
    "            return VerificationResult(\n",
    "                verified=True,\n",
    "                score=2,\n",
    "                reason=\"URL reachable and points to a PDF\",\n",
    "                matched_source=\"url\",\n",
    "                matched_id=r.url,\n",
    "                matched_title=None,\n",
    "            )\n",
    "\n",
    "        # Parse HTML title / meta\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        candidates = []\n",
    "        if soup.title and soup.title.get_text(strip=True):\n",
    "            candidates.append(soup.title.get_text(strip=True))\n",
    "\n",
    "        og = soup.find(\"meta\", attrs={\"property\": \"og:title\"})\n",
    "        if og and og.get(\"content\"):\n",
    "            candidates.append(og[\"content\"].strip())\n",
    "\n",
    "        tw = soup.find(\"meta\", attrs={\"name\": \"twitter:title\"})\n",
    "        if tw and tw.get(\"content\"):\n",
    "            candidates.append(tw[\"content\"].strip())\n",
    "\n",
    "        h1 = soup.find(\"h1\")\n",
    "        if h1 and h1.get_text(strip=True):\n",
    "            candidates.append(h1.get_text(strip=True))\n",
    "\n",
    "        candidates = [c for c in candidates if c]\n",
    "        if not candidates:\n",
    "            return VerificationResult(False, 0, \"URL reachable but no title candidates found\", matched_source=\"url\", matched_id=r.url)\n",
    "\n",
    "        best = max(fuzz.token_set_ratio(ref.title, c) for c in candidates)\n",
    "        if best >= 80:\n",
    "            return VerificationResult(\n",
    "                verified=True,\n",
    "                score=3,\n",
    "                reason=f\"URL title match (score={best})\",\n",
    "                matched_source=\"url\",\n",
    "                matched_id=r.url,\n",
    "                matched_title=max(candidates, key=lambda c: fuzz.token_set_ratio(ref.title, c)),\n",
    "            )\n",
    "\n",
    "        return VerificationResult(\n",
    "            verified=False,\n",
    "            score=0,\n",
    "            reason=f\"URL reachable but title mismatch (best score={best})\",\n",
    "            matched_source=\"url\",\n",
    "            matched_id=r.url,\n",
    "            matched_title=max(candidates, key=lambda c: fuzz.token_set_ratio(ref.title, c)),\n",
    "        )\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return VerificationResult(False, 0, f\"URL fetch failed: {e.__class__.__name__}\", matched_source=\"url\", matched_id=url)\n",
    "\n",
    "\n",
    "def verify_reference(ref: ParsedRef) -> VerificationResult:\n",
    "    # 1) DOI check (strongest)\n",
    "    if ref.doi:\n",
    "        ok, meta = crossref_lookup_doi(ref.doi)\n",
    "        if ok:\n",
    "            return VerificationResult(\n",
    "                verified=True,\n",
    "                score=10,\n",
    "                reason=\"DOI verified via Crossref\",\n",
    "                matched_source=\"crossref\",\n",
    "                matched_id=meta.get(\"DOI\"),\n",
    "                matched_title=_safe_title(meta),\n",
    "                matched_doi=meta.get(\"DOI\"),\n",
    "            )\n",
    "        # DOI present but not found is suspicious\n",
    "        # continue with title search as a fallback\n",
    "\n",
    "    # 2) Title search (OpenAlex + Crossref)\n",
    "    evidence_score = 0\n",
    "    reasons = []\n",
    "\n",
    "    best_match = None  # (source, id, title, doi, title_score, year_ok, author_ok)\n",
    "    if ref.title:\n",
    "        oa = openalex_search(ref)\n",
    "        if oa:\n",
    "            evidence_score += oa[\"evidence\"]\n",
    "            reasons.append(oa[\"reason\"])\n",
    "            best_match = oa[\"best_match\"]\n",
    "\n",
    "        cr = crossref_search(ref)\n",
    "        if cr:\n",
    "            evidence_score += cr[\"evidence\"]\n",
    "            reasons.append(cr[\"reason\"])\n",
    "            # Keep whichever match has better title similarity\n",
    "            if (best_match is None) or (cr[\"best_match\"][4] > best_match[4]):\n",
    "                best_match = cr[\"best_match\"]\n",
    "\n",
    "    if evidence_score >= MIN_EVIDENCE_SCORE and best_match:\n",
    "        source, mid, mtitle, mdoi, tscore, year_ok, author_ok = best_match\n",
    "        return VerificationResult(\n",
    "            verified=True,\n",
    "            score=evidence_score,\n",
    "            reason=\"; \".join(reasons),\n",
    "            matched_source=source,\n",
    "            matched_id=mid,\n",
    "            matched_title=mtitle,\n",
    "            matched_doi=mdoi,\n",
    "        )\n",
    "\n",
    "    # After DOI lookup fails (or after DB search fails), try URL check\n",
    "    url_result = verify_by_url(ref)\n",
    "    if url_result and url_result.verified:\n",
    "        return url_result\n",
    "\n",
    "    # 3) Not verified\n",
    "    why = \"No confident match found\"\n",
    "    if ref.title is None and ref.doi is None:\n",
    "        why = \"No DOI and title missing (cannot verify reliably)\"\n",
    "    elif ref.doi and not ref.title:\n",
    "        why = \"DOI not found and title missing\"\n",
    "\n",
    "    return VerificationResult(\n",
    "        verified=False,\n",
    "        score=evidence_score,\n",
    "        reason=why + ((\" | \" + \"; \".join(reasons)) if reasons else \"\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def crossref_lookup_doi(doi: str) -> Tuple[bool, Dict[str, Any]]:\n",
    "    # Crossref works endpoint supports /works/{doi}\n",
    "    url = f\"{CROSSREF_WORKS}/{doi}\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=REQUEST_TIMEOUT, headers={\"User-Agent\": \"ref-checker/0.1 (mailto:you@example.com)\"})\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return True, data.get(\"message\", {})\n",
    "        return False, {}\n",
    "    except requests.RequestException:\n",
    "        return False, {}\n",
    "\n",
    "\n",
    "def crossref_search(ref: ParsedRef) -> Optional[Dict[str, Any]]:\n",
    "    q = ref.title.strip()\n",
    "    params = {\"query.bibliographic\": q, \"rows\": 5}\n",
    "    try:\n",
    "        r = requests.get(CROSSREF_WORKS, params=params, timeout=REQUEST_TIMEOUT,\n",
    "                         headers={\"User-Agent\": \"ref-checker/0.1 (mailto:you@example.com)\"})\n",
    "        r.raise_for_status()\n",
    "        items = r.json().get(\"message\", {}).get(\"items\", [])\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "    finally:\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    best = None\n",
    "    for it in items:\n",
    "        mtitle = _safe_title(it)\n",
    "        if not mtitle:\n",
    "            continue\n",
    "        tscore = fuzz.token_set_ratio(ref.title, mtitle)\n",
    "        year_ok = _year_matches(ref.year, it.get(\"issued\", {}))\n",
    "        author_ok = _author_overlap(ref.authors, it.get(\"author\", []))\n",
    "        evidence = 0\n",
    "        if tscore >= TITLE_MATCH_THRESHOLD:\n",
    "            evidence += 1\n",
    "        if year_ok:\n",
    "            evidence += 1\n",
    "        if author_ok:\n",
    "            evidence += 1\n",
    "\n",
    "        cand = (\"crossref\", it.get(\"DOI\"), mtitle, it.get(\"DOI\"), tscore, year_ok, author_ok)\n",
    "        if best is None or cand[4] > best[4]:\n",
    "            best = cand\n",
    "\n",
    "    if not best:\n",
    "        return None\n",
    "\n",
    "    evidence = (1 if best[4] >= TITLE_MATCH_THRESHOLD else 0) + (1 if best[5] else 0) + (1 if best[6] else 0)\n",
    "    return {\n",
    "        \"evidence\": evidence,\n",
    "        \"reason\": f\"Crossref best title score={best[4]}, year_ok={best[5]}, author_ok={best[6]}\",\n",
    "        \"best_match\": best,\n",
    "    }\n",
    "\n",
    "\n",
    "def openalex_search(ref: ParsedRef) -> Optional[Dict[str, Any]]:\n",
    "    # OpenAlex: use filter=title.search: and optional year\n",
    "    params = {\n",
    "        \"search\": ref.title,\n",
    "        \"per-page\": 5,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(OPENALEX_WORKS, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        results = r.json().get(\"results\", [])\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "    finally:\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    best = None\n",
    "    for it in results:\n",
    "        mtitle = it.get(\"title\")\n",
    "        if not mtitle:\n",
    "            continue\n",
    "        tscore = fuzz.token_set_ratio(ref.title, mtitle)\n",
    "        year_ok = (ref.year is None) or (it.get(\"publication_year\") == ref.year)\n",
    "        author_ok = _openalex_author_overlap(ref.authors, it.get(\"authorships\", []))\n",
    "        evidence = 0\n",
    "        if tscore >= TITLE_MATCH_THRESHOLD:\n",
    "            evidence += 1\n",
    "        if year_ok:\n",
    "            evidence += 1\n",
    "        if author_ok:\n",
    "            evidence += 1\n",
    "\n",
    "        mid = it.get(\"id\")\n",
    "        doi = it.get(\"doi\")\n",
    "        if doi:\n",
    "            doi = doi.replace(\"https://doi.org/\", \"\").lower()\n",
    "\n",
    "        cand = (\"openalex\", mid, mtitle, doi, tscore, year_ok, author_ok)\n",
    "        if best is None or cand[4] > best[4]:\n",
    "            best = cand\n",
    "\n",
    "    if not best:\n",
    "        return None\n",
    "\n",
    "    evidence = (1 if best[4] >= TITLE_MATCH_THRESHOLD else 0) + (1 if best[5] else 0) + (1 if best[6] else 0)\n",
    "    return {\n",
    "        \"evidence\": evidence,\n",
    "        \"reason\": f\"OpenAlex best title score={best[4]}, year_ok={best[5]}, author_ok={best[6]}\",\n",
    "        \"best_match\": best,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_folder(pdf_dir: str, out_json: str = \"report.json\") -> Dict[str, Any]:\n",
    "    report = {\"suspicious\": [], \"files\": {}}\n",
    "\n",
    "    pdfs = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.lower().endswith(\".pdf\")]\n",
    "    for pdf_path in tqdm(pdfs, desc=\"Processing PDFs\"):\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        try:\n",
    "            refs = grobid_extract_references(pdf_path)\n",
    "        except Exception as e:\n",
    "            report[\"files\"][filename] = {\"error\": str(e), \"refs\": []}\n",
    "            continue\n",
    "\n",
    "        flagged = []\n",
    "        ref_results = []\n",
    "\n",
    "        for ref in refs:\n",
    "            vr = verify_reference(ref)\n",
    "            ref_results.append({\"ref\": asdict(ref), \"verification\": asdict(vr)})\n",
    "            if not vr.verified:\n",
    "                flagged.append({\"ref\": asdict(ref), \"why\": asdict(vr)})\n",
    "\n",
    "        report[\"files\"][filename] = {\n",
    "            \"num_refs\": len(refs),\n",
    "            \"num_unverified\": len(flagged),\n",
    "            \"unverified\": flagged[:],  # you can truncate if you want\n",
    "        }\n",
    "\n",
    "        if flagged:\n",
    "            report[\"suspicious\"].append({\n",
    "                \"filename\": filename,\n",
    "                \"num_unverified\": len(flagged),\n",
    "                \"unverified\": flagged,\n",
    "            })\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80551501",
   "metadata": {},
   "source": [
    "Here are some helper functions used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f40a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def _first_text(items):\n",
    "    return items[0].strip() if items else None\n",
    "\n",
    "def _parse_year(s: Optional[str]) -> Optional[int]:\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"(19|20)\\d{2}\", s)\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def _safe_title(crossref_item: Dict[str, Any]) -> Optional[str]:\n",
    "    t = crossref_item.get(\"title\")\n",
    "    if isinstance(t, list) and t:\n",
    "        return t[0]\n",
    "    if isinstance(t, str):\n",
    "        return t\n",
    "    return None\n",
    "\n",
    "def _year_matches(ref_year: Optional[int], issued: Dict[str, Any]) -> bool:\n",
    "    if ref_year is None:\n",
    "        return True\n",
    "    parts = issued.get(\"date-parts\")\n",
    "    if not parts or not parts[0]:\n",
    "        return False\n",
    "    return parts[0][0] == ref_year\n",
    "\n",
    "def _author_overlap(ref_authors: List[str], crossref_authors: List[Dict[str, Any]]) -> bool:\n",
    "    if not ref_authors or not crossref_authors:\n",
    "        return False\n",
    "    ref_surnames = {a.split(\",\")[0].strip().lower() for a in ref_authors if a}\n",
    "    cr_surnames = {a.get(\"family\", \"\").strip().lower() for a in crossref_authors if a.get(\"family\")}\n",
    "    return len(ref_surnames & cr_surnames) >= 1\n",
    "\n",
    "def _openalex_author_overlap(ref_authors: List[str], authorships: List[Dict[str, Any]]) -> bool:\n",
    "    if not ref_authors or not authorships:\n",
    "        return False\n",
    "    ref_surnames = {a.split(\",\")[0].strip().lower() for a in ref_authors if a}\n",
    "    oa_surnames = set()\n",
    "    for au in authorships:\n",
    "        name = (au.get(\"author\") or {}).get(\"display_name\") or \"\"\n",
    "        # last token as surname-ish heuristic\n",
    "        parts = name.split()\n",
    "        if parts:\n",
    "            oa_surnames.add(parts[-1].strip().lower())\n",
    "    return len(ref_surnames & oa_surnames) >= 1\n",
    "\n",
    "\n",
    "def report_dict_to_unverified_csv(report: dict, out_csv_path: str = \"unverified_refs.csv\") -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    suspicious = report.get(\"suspicious\") or []\n",
    "    if not suspicious and \"files\" in report:\n",
    "        for filename, finfo in report[\"files\"].items():\n",
    "            if finfo.get(\"unverified\"):\n",
    "                suspicious.append({\"filename\": filename, \"unverified\": finfo[\"unverified\"]})\n",
    "\n",
    "    for item in suspicious:\n",
    "        submission = item.get(\"filename\")\n",
    "        for entry in item.get(\"unverified\", []):\n",
    "            ref = entry.get(\"ref\", {})\n",
    "            why = entry.get(\"why\", {}) or entry.get(\"verification\", {})\n",
    "\n",
    "            rows.append({\n",
    "                \"submission_name\": submission,\n",
    "                \"paper_title\": ref.get(\"title\"),\n",
    "                \"paper_authors\": \"; \".join(ref.get(\"authors\", []) or []),\n",
    "                \"publication_year\": ref.get(\"year\"),\n",
    "                \"doi\": ref.get(\"doi\"),\n",
    "                \"url\": ref.get(\"url\"),\n",
    "                \"reason_for_non_verification\": why.get(\"reason\", \"\"),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37683f31",
   "metadata": {},
   "source": [
    "Now we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3819773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█         | 8/73 [01:57<15:53, 14.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m report = \u001b[43manalyze_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDF_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_json\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreport.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWrote report.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df = report_dict_to_unverified_csv(report, \u001b[33m\"\u001b[39m\u001b[33munverified_refs.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 381\u001b[39m, in \u001b[36manalyze_folder\u001b[39m\u001b[34m(pdf_dir, out_json)\u001b[39m\n\u001b[32m    378\u001b[39m ref_results = []\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     vr = \u001b[43mverify_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     ref_results.append({\u001b[33m\"\u001b[39m\u001b[33mref\u001b[39m\u001b[33m\"\u001b[39m: asdict(ref), \u001b[33m\"\u001b[39m\u001b[33mverification\u001b[39m\u001b[33m\"\u001b[39m: asdict(vr)})\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vr.verified:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 192\u001b[39m, in \u001b[36mverify_reference\u001b[39m\u001b[34m(ref)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mverify_reference\u001b[39m(ref: ParsedRef) -> VerificationResult:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# 1) DOI check (strongest)\u001b[39;00m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ref.doi:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m         ok, meta = \u001b[43mcrossref_lookup_doi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdoi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m VerificationResult(\n\u001b[32m    195\u001b[39m                 verified=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    196\u001b[39m                 score=\u001b[32m10\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m                 matched_doi=meta.get(\u001b[33m\"\u001b[39m\u001b[33mDOI\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    202\u001b[39m             )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mcrossref_lookup_doi\u001b[39m\u001b[34m(doi)\u001b[39m\n\u001b[32m    259\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCROSSREF_WORKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREQUEST_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUser-Agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mref-checker/0.1 (mailto:you@example.com)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    263\u001b[39m         data = r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\urllib3\\connection.py:796\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    793\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    794\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    814\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\urllib3\\connection.py:975\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    973\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\msassrb2\\OneDrive - The University of Manchester\\Pycode\\ReferenceChecker\\env\\Lib\\site-packages\\urllib3\\util\\ssl_.py:461\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "report = analyze_folder(PDF_FOLDER, out_json=\"report.json\")\n",
    "print(\"Wrote report.json\")\n",
    "df = report_dict_to_unverified_csv(report, \"unverified_refs.csv\")\n",
    "print(f\"Wrote {len(df)} rows to unverified_refs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
